<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="icon" type="image/x-icon" href="/assets/imgs/favicon.ico">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Mono:wght@400;700&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="/assets/css/tailwind.css">
    <title>Zodi Chalat - The $100 Movie</title>
  </head>
  <body>
  <nav class="max-w-3xl mx-auto flex justify-between h-10 mt-8">
    <div class="w-2/12 border-double border-4 rounded bg-white hover:bg-yellow-200  border-black leading-9 flex justify-center items-center">
        <a href="/" class="text-center">Zodi <span class="hidden sm:inline-block">Chalat</span></a>
    </div>
    <div class="bg-white border-2 border-black flex justify-between leading-9 w-8/12">
        <a href="/thoughts" class="w-full text-center border-r-2 border-black hover:bg-red-200 ">
            <div>Thoughts</div>
        </a>
        <a href="/startups" class="w-full text-center border-r-2 border-black hover:bg-blue-200 ">
            <div>Startups</div>
        </a>
        <a href="/vault" class="w-full text-center border-r-2 border-black hover:bg-yellow-200 ">
            <div>Vault</div>
        </a>
        <a href="/grobie" class="w-full text-center border-r-2 border-black hover:bg-red-200 ">
            <div>Grobie</div>
        </a>
        <a href="/about" class="w-full text-center hover:bg-blue-200 ">
            <div>About Me</div>
        </a>
    </div>
</nav>

<!-- <nav class="max-w-3xl mx-auto bg-red-500 flex justify-between h-10">
    <div class="border-double border-4 rounded bg-white  border-black leading-9">
        <a href="/">Zodi Chalat</a>
    </div>
    <div class="bg-white border-2 border-black flex justify-between min-w-96 bg-red-500 leading-9"><div class="w-20 text-center border-r-2 border-black ">
                <a href="/grobie/">Grobie</a>
            </div><div class="w-20 text-center border-r-2 border-black ">
                <a href="/about/">About Me</a>
            </div><div class="w-20 text-center border-r-2 border-black ">
                <a href="/startups/">Startups</a>
            </div><div class="w-20 text-center border-r-2 border-black ">
                <a href="/vault/">Vault</a>
            </div><div class="w-20 text-center border-r-2 border-black ">
                <a href="/thoughts/">Thoughts</a>
            </div><div class="w-20 text-center border-r-2 border-black ">
                <a href="/">Home</a>
            </div></div>
</nav> -->
  <main class="max-w-3xl mx-auto my-8 text-left"> 
    
<article class="prose lg:prose-xl prose-a:text-blue-500 prose-a:underline">
    <h1>The $100 Movie: How AI is Transforming the Film Industry</h1>
<p><img src="/assets/movie/deniro.jpg" alt="Robert De Niro"></p>
<p>AI for film production is improving rapidly — certainly a lot faster than James Cameron making an Avatar sequel.</p>
<p>If you watched <em>Indiana Jones: Dial of Destiny</em> or <em>The Irishman</em> recently, you saw one of the more presently viable use cases for AI in film — de-aging your dad’s favorite actors like <a href="https://www.wired.com/story/indiana-jones-and-the-dial-of-destiny-de-aging-tech/">Harrison Ford</a> and <a href="https://www.wired.com/story/the-irishman-netflix-ilm-de-aging/">Robert De Niro</a>. And though these breakthroughs in AI might have taken a job away from a young De Niro lookalike, the millions of dollars spent on VFX teams to create this effect shows that maybe some of the <a href="https://www.wired.com/story/hollywood-actors-artificial-intelligence-performance/">AI replacement fears in Hollywood</a> have not quite come true yet. Though de-aging and face-swapping in contemporary films is attributed to AI, if we dig a little deeper most of it takes place not at sleek new Silicon Valley tech companies, but at <a href="https://www.ilm.com/">ILM</a>, a company started by George Lucas that employees hundreds of creatives. The process, needless to say, remains high touch.</p>
<p>But that might not be true for long.</p>
<p>Then RunwayML <a href="https://twitter.com/runwayml/status/1666429706932043776?ref_src=twsrc%5Etfw%7Ctwcamp%5Etweetembed%7Ctwterm%5E1666429706932043776%7Ctwgr%5E837d8cdd6eab88b1ec6539482ef424e94aae6668%7Ctwcon%5Es1_&amp;ref_url=https%3A%2F%2F80.lv%2Farticles%2Frunway-s-gen-2-has-been-officially-released%2F">released</a> Gen-2, a multimodal AI system can generate videos from text, images, and video clips, and suddenly we saw a deluge of AI-generated video clips. The <a href="https://twitter.com/rowancheung/status/1729009694025355475">best of these</a> look like they would take an experienced animator hours or days to recreate. HeyGen <a href="https://the-decoder.com/heygen-offers-ai-powered-video-translation-with-impressive-lip-syncing-capabilities/">released</a> a video translation and lip reanimation tool that produced shockingly natural outputs. And just recently Stability AI <a href="https://stability.ai/news/stable-video-diffusion-open-ai-video-model">open-sourced</a> their own video model, on top of which we can expect a huge number of applications to be built in the coming months, the same way their image generation model created an explosion of new fine-tuned image generation software.</p>
<p>We have seen several times over how new creative tools empower new groups of people to branch out from consumer to creator, which then serves as the engine for new media platforms. We saw this with YouTube and the Adobe creative suite. Suddenly this platform could be populated with high quality short films produced by people in their own homes. But most people did not become YouTubers nor did they learn how to use Premier. I think the adoption of these tools relative to the consumer base is reflective of how difficult they are to use. Even in it’s mature state, there are a limited number of successful creators on YouTube, and content production tends to still be quite expensive. Newer UGC (user generated content) platforms like TikTok have created an ecosystem where a much larger number of creators can be much, much larger. This seems in line with the relatively lower barrier of entry. As new AI tools make it possible to produce high quality content more cheaply and easily, it is likely that it will empower an even larger number of creators to bring their thoughts and ideas to life in film.</p>
<p>There are a huge number of places where AI will touch the film industry, from screenwriting to production financing, to casting, and everything else that happens off screen. For this post, I want to explore specifically how AI-powered creative tools can empower indie film makers, meaning many of the production tools are out of scope. For this domain, I see five areas where startups are using AI to make film production easier, ranked from most currently functional to most in development: audio, animation, editing, and video generation.</p>
<h3>Audio</h3>
<p>Audio is an expensive and complicated part of making a film. There are two distinct areas of audio that are both quite costly — scoring and voice acting. For an indie film, both <a href="https://www.cinemagicscoring.com/post/the-price-of-movie-magic-what-goes-into-scoring-a-film">score</a> and <a href="https://voice123.com/blog/hiring-freelancers/how-much-does-it-cost-to-hire-a-voice-actor/">voice actors</a> come with price tags in the thousands. Using traditional tooling, doing either without hiring a profession is difficult and time consuming. I’ll skip some of the AI scoring solutions, since <a href="https://a16z.com/the-future-of-music-how-generative-ai-is-transforming-the-music-industry/">AI for music generation</a> has been well-covered elsewhere. For voice generation, we have text to speech models. Though text to speech has been around for <a href="https://speechify.com/blog/history-of-text-to-speech/?landing_url=https%3A%2F%2Fspeechify.com%2Fblog%2Fhistory-of-text-to-speech%2F">over 50 years</a>, it is only with recent advances in generative AI that the “robot voice” problem has been solved. Startups like <a href="https://elevenlabs.io/">ElevenLabs</a> and <a href="https://neosapience.com/">Neosapience</a> have built extremely realistic text to speech models that are nearly indistinguishable from human voice (the giveaway is mainly in occasionally unnatural inflection). Both can emulate existing voices very well from only a few minutes of sample. ElevenLabs is focused mainly on selling into the application layer, whereas Neosapience has focused on building tooling for the film industry. Although the highest quality text to speech remains prohibitively expensive for conversational apps, the cost of inference for the 1-2 hours of speech need for a film is minimal. Through ElevenLabs this can currently be done for free.</p>
<p><img src="/assets/movie/ElevenLabs.png" alt="ElevenLabs.png"></p>
<div class="w-full">
  <audio controls class="w-full">
    <source src="/assets/movie/ElevenLabsFinnish.mp3" type="audio/mpeg">
    Your browser does not support the audio element.
  </audio>
</div>
<p>(ElevenLabs speaking my favorite language, Finnish)</p>
<p>An important use of voice acting is not just for creating original content, but also for dubbing content in foreign languages. Dubbing historically has been an expensive process, requiring translators, voice actors, and audio engineers to produce and embed dialogue in another language. And even in the best case scenario, in a live action film dubbed with traditional methods the speech does not match the actors mouths. Though in theory VFX artists could reanimate actors’ mouths to match the new language, it is never done because it is so high cost and many viewers are already somewhat acclimated to watching mismatched audio. Companies like <a href="https://deepdub.ai/">Deepdub</a> and <a href="https://www.google.com/search?q=heygen+ai&amp;oq=heygen&amp;gs_lcrp=EgZjaHJvbWUqEggAEAAYFBiDARiHAhixAxiABDISCAAQABgUGIMBGIcCGLEDGIAEMgwIARBFGDkYsQMYgAQyDwgCEAAYFBiHAhixAxiABDIHCAMQABiABDIHCAQQABiABDIHCAUQABiABDIHCAYQABiABDIHCAcQABiABDIHCAgQABiABDIHCAkQABiABNIBBzcxM2owajSoAgCwAgA&amp;sourceid=chrome&amp;ie=UTF-8">HeyGen</a> offer a solution to this. Both are end-to-end tools for “immersive dubbing”, meaning you can upload a video, and the audio will automatically be translated to the target language using the original voice, and the mouths in video will be reanimated to match the audio. The demos are compelling, but trying it on a video of yourself is truly strange, like seeing yourself learn a new language fluently in a few minutes. For creators just putting content on platforms like YouTube or Vimeo, dubbing is probably an unrealistic expense. These tools may massively increase distribution potential. (There is another interesting part of film audio called foley, but I’ll cover that in a later post).</p>
<h3>Animation</h3>
<p>Before getting into full-blown AI-generated animations, which is still not quite ready for long form video, it’s worth looking at how AI is being leveraged already in modern animators’ workflow. There are two interesting things to look at: background generation and motion capture. In most animation studios, background design is usually handled by a separate team, who spends a lot of time designing and drawing the background scene for every shot in the film. These are typically static art on top of which the moving characters and objects are superimposed. For indie animators, creating background art for every scene is an extremely time-consuming process, which can enormously sped up with image generation. Midjourney, Dall-E, and Stability already produce results that are indistinguishable from big budget animated films.</p>
<p><img src="/assets/movie/frogs.png" alt="frogs.png"></p>
<p>(Whether you are an artist or not, it’s easy to imagine how much additional time the background art here would take to produce without AI. This example in particular was made with Dall-E.)</p>
<p>The other interesting area is in motion capture. For 3D animation, it is often cheaper and produces much higher results to animate characters based on the physical movements of a human in a mo-cap studio, rather than animating the wireframe by hand. This is standard practice now for most major animation studios, but the technology remains very expensive. A high quality studio can cost over $100k to set up, and rentals usually go for around a thousand bucks an hour. Recent advances in computer allow us to generate these same wireframes from just a single video, shot on a phone. The results of companies like <a href="https://www.move.ai/">Move</a> and <a href="https://wonderdynamics.com/#null">Wonder Dynamics</a> are astounding. Move is more oriented towards animated content and gaming, while Wonder Dynamics is focused on CGI for live action. Both highlight how a character model can be animated in very complex ways with minimal effort.</p>
<h3>Tool Editing</h3>
<p>I think it’s worth drawing a distinction between “tool editors” and “prompt editors”. The former is the traditional digital workstation like iMove or After Effects. The software gives you an enormous arsenal of virtualized tools which allow you to change video footage by clicking around in the frames. Machine learning models have already made these tools infinitely more powerful. Things like rotoscoping, object deletion, and match cutting have existed in these tool editors well before they were integrated with ML techniques, but using them required extremely skilled artists to spend hundreds of hours working at a very granular level — often editing frame-by-frame. Traditional computer vision methods have made these tools much more effective, for example finding good candidate shots for transitions at <a href="https://arxiv.org/abs/2210.05766">Netflix</a>, or <a href="https://creativecloud.adobe.com/learn/after-effects/web/rotoscope-subject">isolating actors</a> in After Effects. Similarly, generative AI can make many of these tools even more effective. Updates to <a href="https://www.adobe.com/products/premiere/ai-video-editing.html">After Effects</a> use generative models for denoising, text-based video editing (i.e. cutting a video just by editing the transcript), and color correction. Although these improvements save sophisticated users a great deal of time, there is still a significant learning curve to using these tools effectively. AI-native editing tool startups like RunwayML have simplified this process while still delivering professional-level results. They recently gained some attention after <a href="https://www.axios.com/2023/05/05/runway-generative-ai-chatgpt-video">being used</a> to create profession-level visuals in indie smash hit “Everything Everywhere All At Once.” Most of Runway’s current offering, like Adobe is tool-based. This is contrasted with a new modality of video editor, “prompt editors”, where users upload a video and describe in natural language the way they would like to change it.</p>
<p><img src="/assets/movie/adobe.png" alt="adobe.png"></p>
<p>(The powerful, but incredibly complex Adobe AfterEffects interface)</p>
<h3>Video Generation/Prompt Editing</h3>
<p>There is a spectrum from simple prompt editing to full-blown video generation that reflects the amount of visual input required from the user to turn written video description into satisfactory results. The simplest prompt editing looks something like adding motion or camera movement to a still image. At the other end of the spectrum is aesthetic, photorealistic, long form, multi-shot video generation. The former is already here, although so far has been limited to producing clips a few seconds in length and for the most part the results are still not excellent. The latter will likely not arrive for some time. It’s worth playing with tools like Genmo, Runway, Pika, and Stability to see what the future holds for video generation, but it’s hard to imagine producing a film this way any time soon.</p>
<p><img src="/assets/movie/runwayml.png" alt="runwayml.png"></p>
<p>(Animating a video in RunwayML)</p>
<h3>The Future</h3>
<p>There are two key components to thinking about how AI will change the film industry: production and consumption. I’ve mostly covered production in this post. New technologies enable creators to make content faster, more cheaply, and with less specialized knowledge and equipment. It is already starting to level the playing field, and soon enough creators with no domain expertise will be able to make films that, at least aesthetically, match films being made <em>today.</em> That being said, I believe people will always find clever ways to produce box office hits with nine figure budgets. One of the ways this might happen is with new consumption modalities for film. Imagine how watching a film could become a two-way street. There are ways to make films more interactive now (think choose-your-own-adventure films, or even heavily narrative-driven video games), but they are clunky and very expensive to make. They are also generally quite limited to a predetermined set of experiences. What would it look for films to have rich interactive experiences, where the viewer is deeply embedded both within the narrative and in the process of creating it. It’s hard to know exactly what this could look like and what elements of this will be sticky, but I find it the most exciting frontier of filmmaking we are likely to pass in the next decade.</p>

</article>

  <script src="/assets/js/script.js"></script>
  </main>
  </body>
</html>