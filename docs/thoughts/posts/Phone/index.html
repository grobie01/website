<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="icon" type="image/x-icon" href="/assets/imgs/favicon.ico">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Mono:wght@400;700&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="/assets/css/tailwind.css">
    <title>Zodi Chalat - Building a Local AI Landline Phone</title>
  </head>
  <body>
  <nav class="max-w-3xl mx-auto flex justify-between h-10 mt-8">
    <div class="w-2/12 border-double border-4 rounded bg-white hover:bg-yellow-200  border-black leading-9 flex justify-center items-center">
        <a href="/" class="text-center">Zodi <span class="hidden sm:inline-block">Chalat</span></a>
    </div>
    <div class="bg-white border-2 border-black flex justify-between leading-9 w-8/12">
        <a href="/thoughts" class="w-full text-center border-r-2 border-black hover:bg-red-200 ">
            <div>Thoughts</div>
        </a>
        <a href="/startups" class="w-full text-center border-r-2 border-black hover:bg-blue-200 ">
            <div>Startups</div>
        </a>
        <a href="/vault" class="w-full text-center border-r-2 border-black hover:bg-yellow-200 ">
            <div>Vault</div>
        </a>
        <a href="/grobie" class="w-full text-center border-r-2 border-black hover:bg-red-200 ">
            <div>Grobie</div>
        </a>
        <a href="/about" class="w-full text-center hover:bg-blue-200 ">
            <div>About Me</div>
        </a>
    </div>
</nav>

<!-- <nav class="max-w-3xl mx-auto bg-red-500 flex justify-between h-10">
    <div class="border-double border-4 rounded bg-white  border-black leading-9">
        <a href="/">Zodi Chalat</a>
    </div>
    <div class="bg-white border-2 border-black flex justify-between min-w-96 bg-red-500 leading-9"><div class="w-20 text-center border-r-2 border-black ">
                <a href="/grobie/">Grobie</a>
            </div><div class="w-20 text-center border-r-2 border-black ">
                <a href="/about/">About Me</a>
            </div><div class="w-20 text-center border-r-2 border-black ">
                <a href="/startups/">Startups</a>
            </div><div class="w-20 text-center border-r-2 border-black ">
                <a href="/vault/">Vault</a>
            </div><div class="w-20 text-center border-r-2 border-black ">
                <a href="/thoughts/">Thoughts</a>
            </div><div class="w-20 text-center border-r-2 border-black ">
                <a href="/">Home</a>
            </div></div>
</nav> -->
  <main class="max-w-3xl mx-auto my-8 text-left"> 
    
<article class="prose lg:prose-xl prose-a:text-blue-500 prose-a:underline">
    <h1>Building a Local AI Landline Phone</h1>
<p><img src="/assets/imgs/phone.png" alt="phone.png"></p>
<p>One day a year, venture firms take a day off from meeting founders to instead tell their investors all about the work they do. These “annual general meetings” are important because they help inform whether shareholders want to continue investing in future funds. Here’s something you don’t want — the associate who’s been at the firm for two weeks answering questions about things he’s not quite up to speed on. If only venture firms had the equivalent of a call center’s “let me transfer you to a representative who can help you”. In order to prepare for this year’s annual meeting, I built just that — a landline phone investors can pick up and talk to an AI who knows everything worth sharing about our fund. Here’s the challenge, we can’t send private portfolio data to OpenAI or 11Labs, so this all needed to run locally. This project ended up being a very fun way to see how quickly you can stitch together local models that run surprisingly quickly. I drew a lot of inspiration from <a href="https://github.com/mezbaul-h/june">june</a>, so definitely check that out! Alright, let’s get into it.</p>
<p>A voice chatbot fundamentally relies on three component models: speech-to-text, text-to-text (i.e. an LLM), and text-to-speech. This will get more complicated later on, but for now let’s focus on these three. We can make a class for each one:</p>
<h3>Speech-to-text</h3>
<p>We'll use the pipeline from the huggingface library which let's you implement local models with a simple API.</p>
<pre><code class="language-python">from transformers import pipeline

class STT:
    def __init__(self, model_name=&quot;openai/whisper-small&quot;):
        self.model_name = model_name

        self.model = pipeline(
            &quot;automatic-speech-recognition&quot;,
            chunk_length_s=12,
            device=&quot;cuda&quot;,
            model=self.model_name,
            token=&quot;...&quot;,
            trust_remote_code=True)

    def transcribe(self, audio):
        return self.model(audio)[&quot;text&quot;].strip()
</code></pre>
<h3>Text-to-text (LLM)</h3>
<p>This is where we set up the interaction with the locally-running language model. Ollama makes this super easy. I'm running Nvidia's newest model Nemotron on our mac mini with 64GB of memory, but it's a bit too slow if you're running it just on a laptop.</p>
<pre><code class="language-python">from ollama import Client

class LLM:
    def __init__(self, model_name, system_prompt):

        self.messages = [] #List(Dict(str: str))

        self.model_name = model_name
        self.system_prompt = system_prompt
        self.model = Client()

    def generate(self, input):
        self.history.append({&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: input})

        role = None
        content = &quot;&quot;

        stream = self.model.chat(
            model=self.model_name,
            messages=self.messages,
            stream=True)

        for chunk in stream:
            token = chunk[&quot;message&quot;][&quot;content&quot;]

            if role is None:
                role = chunk[&quot;message&quot;][&quot;role&quot;]

            content += token

            yield token

        self.messages.append({&quot;role&quot;: role, &quot;content&quot;: content})
        return content
</code></pre>
<h3>Text-to-speech</h3>
<p>This is where we convert the text to speech. This is the only step which is not just slower when runinng locally, but also noticibly lower output quality. That said, for English it's suprisingly good. Python has a library called TTS so you don't have to do much to get this running.</p>
<pre><code class="language-python">from TTS.api import TTS as TTSAPI

class TTS:
    def __init__(self):
        self.model = TTSAPI()

        self.device = &quot;cuda&quot;

        self.model = TTSAPI(&quot;tts_models/multilingual/multi-dataset/xtts_v2&quot;).to(self.device)

    def generate(self, text):
        return self.model.generate(text=text, split_sentences=False)
</code></pre>
<h2>Audio</h2>
<p>Here is where we hit our first speed bump. I had a chatbot that ran fine on my local machine, but then I realized I had no idea how to get it to run on an analogue phone. The first idea was to use bluetooth. The first approach was to use <a href="https://www.amazon.com/Xtreme-Technolgoies-XLink-Bluetooth-Gateway/dp/B08RXF16XD">this device</a> which is a bluetooth adapter that connects to an RJ11 input and then transmits audio over bluetooth to a phone. This worked, but it wasn't great. Landline phones are not just in &quot;recording mode&quot; at all times. When you pick up the phone, it's makes a dial tone, and you have to wait for a connection to actually start recording. This is getting outside the land of high level software, so I didn't know if I could jerry rig the phone to automatically go into recording mode. What I could do is send a bluetooth signal that might trick it into thinking it's on a call. After reading this <a href="https://people.csail.mit.edu/albert/bluez-intro/index.html">awesome guide</a> on bluetooth programming I came to the conclusion that this was possible (yay) but not with a version of python that let me use all the other helpful ML libraries. So I decided to ignore the problem, and just focus on building somethign that kind of worked.</p>
<p>You can get really fancy with how you treat always-on audio, but I went with a simple approach that just constantly analyzes the last few seconds of audio data to see if it's silent. If it is, the loop continues, if it's not, it records the full audio.</p>
<pre><code class="language-python">def is_silent(data: np.ndarray) -&gt; bool:
        &quot;&quot;&quot;Check if the given audio data is silent based on the threshold.&quot;&quot;&quot;
        return np.max(data) &lt; AudioIO.THRESHOLD

    def record_audio(self) -&gt; Optional[Dict[str, Union[int, np.ndarray]]]:
        &quot;&quot;&quot;
        Record audio from the microphone until silence is detected.

        Returns:
            A dictionary containing the recorded audio data and the sampling rate,
            or None if no audio was recorded.
        &quot;&quot;&quot;
        if not self.input_stream:
            self._initialize_input_stream()

        frames: List[np.ndarray] = []
        current_silence = 0
        recording = False

        print(&quot;Listening for sound...&quot;)

        while True:
            # Read audio data from input stream
            data = np.frombuffer(self.input_stream.read(self.CHUNK), dtype=np.int16)

            if not recording and not self.is_silent(data):
                print(&quot;Sound detected, starting recording...&quot;)
                recording = True

            if recording:
                frames.append(data)

                if self.is_silent(data):
                    current_silence += 1
                else:
                    current_silence = 0

                # Stop recording after detecting sufficient silence
                if current_silence &gt; (self.SILENCE_LIMIT * self.RATE / self.CHUNK):
                    print(&quot;Silence detected, stopping recording...&quot;)
                    break

        # Stop the audio stream
        self.input_stream.stop_stream()

        if recording:
            # Concatenate all recorded frames into a single numpy array
            raw_data = np.hstack(frames)

            # Normalize the recorded data for compatibility with STT models
            normalized_data = raw_data.astype(np.float32) / np.iinfo(np.int16).max

            return {
                &quot;raw&quot;: normalized_data,
                &quot;sampling_rate&quot;: self.RATE,
            }
        else:
            return None
</code></pre>
<p>Finally, we stitch it all together in our main function:</p>
<pre><code class="language-python">def main():
    llm_model = LLM(model_name=&quot;nemotron&quot;, system_prompt=&quot;You are a helpful assistant living in the Root Ventures office&quot;)
    stt_model = STT(model_name=&quot;openai/whisper-small&quot;)
    tts_model = TTS()

    text_queue = asyncio.Queue()
    thread = Thread(target=run_async_tasks, args=(text_queue, tts_model))
    thread.start()

    try:
        producer(text_queue, llm_model, stt_model, current_app_state)  # Pass current_app_state explicitly
    finally:
        shutdown_event.set()
        thread.join()
        asyncio.run(_clear_queue(text_queue))
</code></pre>
<p>Great, we have a working chatbot! Unfortunately the phone only works half the time, so we'd have to figure out a better way to to triger the audio recording. The solution ended up being disappointingly simple. The phone itself is just a speaker and microphone, and the processing happens on a chip in the phone base. We can actually connect the RJ9 output of the phone to a RJ9-to-3.5mm jack adapter, and then it's just like using the microphone on my computer.</p>
<p>We're almost done. The only thing left is to get all our data available to the model. One option is to just load up the system prompt, which worked well enough for the first runs, but in order to read all of our data, we need to build in RAG. For my use case, <a href="https://huggingface.co/learn/cookbook/en/advanced_rag">this cookbook</a> is mostly sufficient, but if I want to add in more complex data like slide decks, graphics, spreadsheets, etc. I'll need a more complex data ingestion system. This was a super fun project, and ultimately was suprised by how little code it takes to build something like this. One final takeaway is that I'm surprised by how many adapter cables exist online. Surely some of these are made cynically to sell to consumers who are searching for the wrong thing (like the notoriously lethal male to male extension cord). This reminds me of the dying industry of knockoff movies, where films like <a href="https://www.imdb.com/title/tt1256535/">Ratatoing</a> were made to just in the hopes that people would buy it thinking it was the real Ratatouille.</p>

</article>

  <script src="/assets/js/script.js"></script>
  </main>
  </body>
</html>